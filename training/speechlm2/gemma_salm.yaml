# Adjusted version of salm.yaml example from NeMo repository.
# https://github.com/NVIDIA/NeMo/blob/main/examples/speechlm2/conf/salm.yaml

model:
  # Every name/path here starting with 'pretrained' is used to initialize the model weights.
  pretrained_llm: cjvt/GaMS-9B
  pretrained_asr: aklemen/slovene-conformer-ctc

  pretrained_weights: True

  # Regexp (re.compile) patterns matching parameters to be frozen.
  freeze_params:
    # Frozen LLM
    - "^llm\\..+$"  # LLM
    - "^embed_tokens\\..+$"  # LLM embedding is moved
    # Frozen pretrained ASR (only the modality adapter layers are trainable)
    - "^perception\\.preprocessor\\..+$"
    - "^perception\\.encoder\\..+$"
  prevent_freeze_params: [ ]  # Use to make specific submodules trainable; overrides freeze_params

  prompt_format: gemma
  audio_locator_tag: "<audio>"  # placeholder token for audio turn is expected

  # Note: Uncomment the block below to enable LoRA on LLM via HuggingFace PEFT library.
  #   It will automatically freeze LLM parameters even if freeze_params was unused,
  #   and prevent freezing any parameter that has the string '.lora_' in its name.
  lora:
    task_type: CAUSAL_LM
    r: 8
    lora_alpha: 32
    lora_dropout: 0.1

  perception:
    target: nemo.collections.speechlm2.modules.perception.AudioPerceptionModule
    modality_adapter:
      _target_: nemo.collections.asr.modules.ConformerEncoder
      feat_in: 512
      feat_out: -1 # you may set it if you need different output size other than the default d_model
      n_layers: 2
      d_model: 1024
      subsampling: dw_striding # vggnet, striding, stacking or stacking_norm, dw_striding
      subsampling_factor: 1 # must be power of 2 for striding and vggnet
      subsampling_conv_channels: 256 # set to -1 to make it equal to the d_model
      causal_downsampling: false
      ff_expansion_factor: 4
      self_attention_model: rel_pos # rel_pos or abs_pos
      n_heads: 8 # may need to be lower for smaller d_models
      # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention
      att_context_size: [ -1, -1 ]
      att_context_style: regular # regular or chunked_limited
      xscaling: true # scales up the input embeddings by sqrt(d_model)
      untie_biases: true # unties the biases of the TransformerXL layers
      pos_emb_max_len: 5000
      conv_kernel_size: 9
      conv_norm_type: batch_norm # batch_norm or layer_norm or groupnormN (N specifies the number of groups)
      # conv_context_size can be"causal" or a list of two integers while conv_context_size[0]+conv_context_size[1]+1==conv_kernel_size
      # null means [(kernel_size-1)//2, (kernel_size-1)//2], and 'causal' means [(kernel_size-1), 0]
      conv_context_size: null
      ### regularization
      dropout: 0 # The dropout used in most of the Conformer Modules
      dropout_pre_encoder: 0 # The dropout used before the encoder
      dropout_emb: 0.0 # The dropout used for embeddings
      dropout_att: 0 # The dropout for multi-headed attention modules

  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-4
    betas: [ 0.9, 0.98 ]
    weight_decay: 1e-3
    foreach: true # set to false if having issues with tensor-parallelism

  lr_scheduler:
    _target_: nemo.core.optim.lr_scheduler.CosineAnnealing
    warmup_steps: 2000
    min_lr: 1e-6
    max_steps: ${trainer.max_steps}

trainer:
  devices: auto
  accelerator: gpu
  num_nodes: 1
  precision: bf16-true
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  use_distributed_sampler: False
  max_steps: 100000
  val_check_interval: 3000
  log_every_n_steps: 10
  num_sanity_val_steps: 1
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  strategy:
    _target_: lightning.pytorch.strategies.DDPStrategy
    gradient_as_bucket_view: true
    find_unused_parameters: true

data:
  train_ds:
    sample_rate: 16000
    prompt_format: ${model.prompt_format}
    token_equivalent_duration: 0.08
    input_cfg:
      - type: lhotse_as_conversation
        shar_path: /custom-datasets/artur/clean/lhotse/shar/train
        audio_locator_tag: ${model.audio_locator_tag}
        tags:
          context: "Transkribiraj naslednji posnetek:"
    seed: 42
    shuffle: true
    shard_seed: "randomized"
    num_workers: 1
    batch_size: 8
    # Optional bucketing:
    # batch_size: null
    # use_bucketing: true
    # use_multimodal_sampling: true
    # measure_total_length: true
    # Note: `batch_tokens`, `bucket_duration_bins`, and `max_tokens` all represent tokens as
    #  the sum of input audio frames and output text tokens. Number of audio frames is
    #  calculated using `token_equivalent_duration`.
    # batch_tokens: 4000
    # max_tokens: 2048
    # bucket_duration_bins: [64, 128, 256, 384, 512, 768, 1024, 1280, 1536, 2048]
    # num_buckets: 10
    # bucket_buffer_size: 5000

  validation_ds:
    # The entries under 'datasets' are a list of separate dataloaders.
    # The structure is <dataset-name>: {<dataloader-dict-config>}
    # They inherit all settings from validation_ds, but can individually override them.
    prompt_format: ${model.prompt_format}
    token_equivalent_duration: 0.08
    datasets:
      artur_clean_val:
        input_cfg:
          - type: lhotse_as_conversation
            shar_path: /custom-datasets/artur/clean/lhotse/shar/val
            audio_locator_tag: ${model.audio_locator_tag}
            tags:
              context: "Transkribiraj naslednji posnetek:"
    sample_rate: 16000
    batch_size: 1
    seed: 42
    shard_seed: "randomized"

exp_manager:
  explicit_log_dir: /exp/logs
  name: salm
  create_tensorboard_logger: false
  create_checkpoint_callback: true
  create_early_stopping_callback: true
  use_datetime_version: true

  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  # you need to set these two to True to continue the training
  resume_if_exists: true
  resume_ignore_no_checkpoint: true

  # You may use this section to create a W&B logger
  create_wandb_logger: true
  wandb_logger_kwargs:
    name: GaMS-9B-SALM
    project: SpeechLLM
    resume: true

  checkpoint_callback_params:
    filename: "{epoch}-{step}-{val_acc:.2f}"
    monitor: val_acc
    mode: max
    every_n_train_steps: ${trainer.val_check_interval}
    every_n_epochs: null
    save_top_k: 1
    always_save_nemo: false
    save_nemo_on_train_end: false

  early_stopping_callback_params:
    monitor: val_acc
    mode: max
    min_delta: 0.0
    patience: 5
    check_finite: true
    verbose: true
