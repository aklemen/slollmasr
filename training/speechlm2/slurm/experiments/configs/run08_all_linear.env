# Run 08: All Linear LoRA Targets (Phase 4)
# Test LoRA on all linear layers instead of just QV
# TODO: Set FREEZE_ENCODER and adapter params based on Phase 2-3 results
export RUN_NAME="run08_all_linear"
export FREEZE_ENCODER=true  # UPDATE based on Phase 2 results
export ADAPTER_SUBSAMPLING=8  # UPDATE based on Phase 3 results
export ADAPTER_N_LAYERS=2  # UPDATE based on Phase 3 results
export ADAPTER_D_MODEL=1024  # UPDATE based on Phase 3 results
export LORA_RANK=128
export LORA_ALPHA=256
export LORA_TARGET_MODULES='"all-linear"'
export LLM_NAME="cjvt/GaMS-9B"
export PROMPT_FORMAT="gemma"
export WANDB_GROUP="phase4-lora"
export WANDB_TAGS="freeze_encoder:true,subsampling:8,lora_r:128,lora_modules:all_linear"
