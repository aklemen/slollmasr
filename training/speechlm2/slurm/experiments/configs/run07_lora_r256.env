# Run 07: LoRA r=256 (Phase 4)
# Test larger LoRA rank
# TODO: Set FREEZE_ENCODER and adapter params based on Phase 2-3 results
export RUN_NAME="run07_lora_r256"
export FREEZE_ENCODER=true  # UPDATE based on Phase 2 results
export ADAPTER_SUBSAMPLING=8  # UPDATE based on Phase 3 results
export ADAPTER_N_LAYERS=2  # UPDATE based on Phase 3 results
export ADAPTER_D_MODEL=1024  # UPDATE based on Phase 3 results
export LORA_RANK=256
export LORA_ALPHA=256
export LORA_TARGET_MODULES='["q_proj", "v_proj"]'
export LLM_NAME="cjvt/GaMS-9B"
export PROMPT_FORMAT="gemma"
export WANDB_GROUP="phase4-lora"
export WANDB_TAGS="freeze_encoder:true,subsampling:8,lora_r:256,lora_modules:qv"
