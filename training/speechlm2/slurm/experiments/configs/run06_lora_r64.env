# Run 06: LoRA r=64 (Phase 4)
# Test smaller LoRA rank
# TODO: Set FREEZE_ENCODER and adapter params based on Phase 2-3 results
export RUN_NAME="run06_lora_r64"
export FREEZE_ENCODER=true  # UPDATE based on Phase 2 results
export ADAPTER_SUBSAMPLING=8  # UPDATE based on Phase 3 results
export ADAPTER_N_LAYERS=2  # UPDATE based on Phase 3 results
export ADAPTER_D_MODEL=1024  # UPDATE based on Phase 3 results
export LORA_RANK=64
export LORA_ALPHA=256
export LORA_TARGET_MODULES='["q_proj", "v_proj"]'
export LLM_NAME="cjvt/GaMS-9B"
export PROMPT_FORMAT="gemma"
export WANDB_GROUP="phase4-lora"
export WANDB_TAGS="freeze_encoder:true,subsampling:8,lora_r:64,lora_modules:qv"
